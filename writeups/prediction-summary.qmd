---
title: "Predictive modeling of claims status"
author: 'Daniel Yan, Christina Cui, Sophie Shi, Henry Louie'
date: today
---

### Abstract

During the preliminary process we found no significant benefits to including headers in the text classification, nor did we see a significant boost in accuracy of the principal component model that uses bigrams. With these results in mind, our team decided to preprocess our text in a standard format, using the temp_text alone for collection of word predictors, and setting our tokenization to "words". In this case, we directly use the claims_clean in file claims-clean-example.RData to train our model. 

We used TF-IDF to measure the significance of words in each claim observation using the function, bind_tf_idf(). This produced a data set with each row representing a unique claim, having its unique ID, its multi-classification and its binary classification alongside 33063 tokenized words with their attached tfidf for that claim.

Using this preprocessed data set, we proceeded to create two different models, SVM, RNN, and Random Forest, to test multi classification and binary classification on the claims. 

For binary classification, we achieved an accuracy of 81.9% for RNN model, an accuracy of 82.26% for SVM model, and an accuracy of 77.34% for Random Forest Model. And for multi-class classification, we achieved an accuracy of 76.73% for the SVM model.

### Preprocessing

In our preliminary analysis, as we noticed no significant benefits from using either header nor bigram information, we extracted just the body text from the HTMLs.

After, the text was cleaned and all non-letter data was removed leaving us with a clean set of body text from each HTML page - removing things such as punctuation and symbols. Further the data was reduced to all lowercase to ensure homoginity of words that may have been capitalized in some instances. Then the data was tokenized - taking the chunks of text from the HTMLs and reducing them into 1 word tokens to which stop words (words that appear frequently but do not add meaning such as ‘a’ and ‘an’) were removed.

Finally, the frequency of words in each document was calculated and turned into TF-IDF numerically which represents how important that certain token word is to the document it is in across a collection of documents. The TF-IDF values were then used in our model building.

### Methods

#### Binary Classification Models

Our first attempt at finding an optimal model for binary classification of the claims was to use Support Vector Machine (SVM) to analyze and classify the word-tokenized data from the clean-claims-example.Rdata.

After preprocessing the claims data, we partitioned it into training and testing data sets. Additionally, we performed principal component analysis on the data to reduce the dimensionality.

Finally, we modeled the training data onto our SVM using a radial kernel, with parameters cost equals to 1 and gamma equals to 0.1, and then predicted using the projected testing data. We used a confusion matrix to calculate the accuracy as our metric to measure the capabilities of our SVM model, returning 0.8226351, which is a pretty good result.

Our second attempt was using the Recurrent Neural Network (RNN) model to predict the binary classification of the claims. Similarly to when we created the SVM, we preprocessed the claims-clean-example.Rdata and partitioned the claims into a training and testing set. Instead of performing principal components however we directly used the RNN on the data.

We used adaptive moment estimation (Adam) as our optimizer and 50 epochs for the RNN model. With these parameters our model returned a binary accuracy of 0.819.

Our third attempt was using a Random Forest model. After preprocessing the data into TF-IDF features, we trained the Random Forest model with 500 decision trees and `mtry` set to the square root of the number of features. Using the testing dataset, we achieved a binary accuracy of 0.7734, which, although lower than both the SVM and RNN models, still demonstrated the model's ability to capture key patterns in the claims data. Feature importance analysis from the Random Forest model highlighted significant terms that influenced classification, providing interpretable insights into the data.

With these results from our SVM, RNN, and Random Forest models for binary classification, we believe the SVM offers the highest binary accuracy, while the Random Forest model provides valuable feature-level insights. In conclusion, the best model to predict the binary classification of claims is with a recurrent neural network.

#### Multi-Classification Models

Our first attempt at finding an optimal model for multi-class classification of the claims was to use Support Vector Machine (SVM) to analyze and classify the word-tokenized data from the clean-claims-example.Rdata.

After preprocessing the claims data, we partitioned it into training and testing data sets with 70% training and 30% testing data. Additionally, we performed principal component analysis on the data to reduce the dimensionality.

Finally, we modeled the training data onto our SVM using a radial kernel, with parameters cost equals to 1 and gamma equals to 0.1, and then predicted using the projected testing data. We used a confusion matrix to calculate the accuracy as our metric to measure the capabilities of our SVM model, returning 76.73%, which is a pretty good result.

### Results

Indicate the predictive accuracy of the binary classifications and the multiclass classifications. Provide a table for each, and report sensitivity, specificity, and accuracy.[^1]

For SVM binary prediction:
```{r, echo=FALSE}
load("../results/svm_binary/accuracy_svm_b.RData")
print(acc_df)
```
For SVM Multi-class prediction:
```{r, echo=FALSE}
load("../results/svm_multi/accuracy_svm_m.RData")
load("../results/svm_multi/sensitivity_specificity.RData")
df <- data.frame(
  model = "SVM Multi-class",
  accuracy = accuracy_svm_m
)
print(df)
print(metrics_df)
```


[^1]: Read [this article](https://yardstick.tidymodels.org/articles/multiclass.html) on multiclass averaging.
